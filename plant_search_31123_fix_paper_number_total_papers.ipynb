{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1s63n17VOwpqA_uZXPOYMTRms4zoDKYu9",
      "authorship_tag": "ABX9TyPZuUIVqaIcS1qc8ummLEfX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GithubUser017/EntrezPlantSearch/blob/main/plant_search_31123_fix_paper_number_total_papers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "x76S5KVkoJbB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba1fc0a-62c9-484c-eb10-dbe61f577acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Bio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDpKxoSC9G88",
        "outputId": "72d6832a-9963-47db-f8c6-3e99085bf3eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Bio\n",
            "  Downloading bio-1.5.6-py3-none-any.whl (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.6/275.6 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from Bio) (4.65.0)\n",
            "Collecting mygene\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from Bio) (2.25.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from Bio) (1.3.5)\n",
            "Collecting biopython>=1.80\n",
            "  Downloading biopython-1.81-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pooch in /usr/local/lib/python3.9/dist-packages (from Bio) (1.7.0)\n",
            "Collecting gprofiler-official\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from biopython>=1.80->Bio) (1.22.4)\n",
            "Collecting biothings-client>=0.2.6\n",
            "  Downloading biothings_client-0.2.6-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->Bio) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas->Bio) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from pooch->Bio) (23.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.9/dist-packages (from pooch->Bio) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->Bio) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->Bio) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->Bio) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->Bio) (2022.12.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7.3->pandas->Bio) (1.15.0)\n",
            "Installing collected packages: biopython, gprofiler-official, biothings-client, mygene, Bio\n",
            "Successfully installed Bio-1.5.6 biopython-1.81 biothings-client-0.2.6 gprofiler-official-1.0.0 mygene-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#experimental trying to make choice 3 work\n",
        "#experimental run from drive \n",
        "\n",
        "\n",
        "from Bio import Entrez\n",
        "import datetime\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "from http.client import IncompleteRead\n",
        "\n",
        "email = input('What is your email address?')\n",
        "# Email address is required by NCBI\n",
        "Entrez.email = email\n",
        "\n",
        "# Choose which txt files to search over\n",
        "choice = input('Do you want to search over all plant genera (enter 1) | all phytochemicals (enter 2) | or both (enter 3) |\\\n",
        "                If you do not want to search over plants or phytochemicals, \\\n",
        "                try searching over human genes first (not as good as MESH term search) (enter 4) : ')\n",
        "choice = int(choice)\n",
        "\n",
        "# Load correct text file\n",
        "if choice == 1:\n",
        "    with open('drive/MyDrive/plant_search_text_files/genus_names2.txt', 'r') as f:\n",
        "        genus_names = f.read().split('@')\n",
        "if choice == 2:\n",
        "    with open('drive/MyDrive/plant_search_text_files/phytochem3.txt', 'r') as f:\n",
        "        genus_names = f.read().split('\\t')\n",
        "       \n",
        "if choice == 3:\n",
        "    with open('drive/MyDrive/plant_search_text_files/genus_names2.txt', 'r') as f:\n",
        "        genus_names = f.read().split('@')\n",
        "    with open('drive/MyDrive/plant_search_text_files/phytochem3.txt', 'r') as f:\n",
        "        phyt_names = f.read().split('\\t')\n",
        "\n",
        "if choice == 4:\n",
        "    with open('drive/MyDrive/plant_search_text_files/gene1.txt', 'r') as f:\n",
        "        genus_names = f.read().split('@')\n",
        "\n",
        "\n",
        "# User-defined search term\n",
        "user_query = input('Enter additional non-plant search terms: ')\n",
        "\n",
        "# Set counter in case choice == 3. This allows first 38 searches to include \"plant\" as a key word\n",
        "gen_phyt_counter = 1\n",
        "\n",
        "# Create directory for input files if it doesn't exist\n",
        "if not os.path.exists('input_files'):\n",
        "    os.makedirs('input_files')\n",
        "\n",
        "# Split the genus names into groups of 1000 or less, to stay under the PubMed search limit\n",
        "genus_groups = [genus_names[i:i+1000] for i in range(0, len(genus_names), 1000)]\n",
        "\n",
        "if choice == 3:\n",
        "    genus_groups = [genus_names[i:i+1000] for i in range(0, len(genus_names), 1000)]\n",
        "    phyt_groups = [phyt_names[i:i+1000] for i in range(0, len(phyt_names), 1000)]\n",
        "\n",
        "    genus_groups = genus_groups + phyt_groups\n",
        "   \n",
        "\n",
        "\n",
        "# List to store abstracts and their associated date information\n",
        "abstracts_with_info = []\n",
        "\n",
        "# Set to keep track of seen Pubmed IDs\n",
        "seen_pmids = set()\n",
        "\n",
        "for i, genus_group in enumerate(genus_groups):\n",
        "    # Construct query string\n",
        "    if choice == 1: \n",
        "        query_terms = '(' + ' OR '.join(genus_group) + ') + AND \"plant\" AND ' + user_query\n",
        "    if choice == 3: \n",
        "        if gen_phyt_counter <= 38:\n",
        "            query_terms = '(' + ' OR '.join(genus_group) + ') + AND \"plant\" AND ' + user_query\n",
        "        if gen_phyt_counter > 38:\n",
        "            query_terms = '(' + ' OR '.join(genus_group) + ') + AND ' + user_query \n",
        "    if choice == 2: \n",
        "        query_terms = '(' + ' OR '.join(genus_group) + ') + AND ' + user_query \n",
        "    if choice == 4: \n",
        "        query_terms = '(' + ' OR '.join(genus_group) + ') + AND ' + user_query\n",
        "    \n",
        "    gen_phyt_counter += 1\n",
        "    \n",
        "    # # # testing line, remove\n",
        "    # if gen_phyt_counter == 38:\n",
        "    #     print(query_terms)\n",
        "    # if gen_phyt_counter == 39:\n",
        "    #     print(query_terms)\n",
        "    # if gen_phyt_counter == 40:\n",
        "    #     print(query_terms)\n",
        "\n",
        "\n",
        "    # Print search query\n",
        "    print(f'Searching group {i+1}/{len(genus_groups)}')\n",
        "\n",
        "    \n",
        "    # Perform search\n",
        "    herror = 0\n",
        "    error_number = 0\n",
        "    while herror == 0:\n",
        "      try:\n",
        "          handle = Entrez.esearch(db='pubmed', term=query_terms, retmax=100000)\n",
        "          record = Entrez.read(handle)\n",
        "          handle.close()\n",
        "          herror = 1\n",
        "      except Exception as err:\n",
        "          error_number += 1\n",
        "          if error_number == 5:\n",
        "            raise err\n",
        "          print(f\"Error: {str(err)}. Retrying in 5 seconds...\")\n",
        "          time.sleep(5)\n",
        "          herror = 0\n",
        "        \n",
        "\n",
        "    # Fetch abstracts for all search results\n",
        "    id_list = record['IdList']\n",
        "    \n",
        "    query_numb = 1\n",
        "    exc = 1\n",
        "    \n",
        "    if id_list:\n",
        "      while exc == 1:\n",
        "        try:\n",
        "          print(f'Fetching {len(id_list)} abstracts...')\n",
        "          handle = Entrez.efetch(db='pubmed', id=id_list, retmode='xml')\n",
        "          records = Entrez.read(handle)\n",
        "          handle.close()\n",
        "          exc = 0\n",
        "        except IncompleteRead:\n",
        "          query_numb += 1\n",
        "          if query_numb == 5:\n",
        "            raise Exception('Failed to fetch abstracts after 5 attempts.')\n",
        "          print(f'Error fetching abstracts, retrying ({query_numb}/5)...')\n",
        "          time.sleep(5) # Wait 5 seconds before retrying\n",
        "          exc = 1\n",
        "          \n",
        "          \n",
        "          \n",
        "\n",
        "    # def fetch_abstracts(id_list):\n",
        "    #   for i in range(5): # Try up to 5 times\n",
        "    #       try:\n",
        "    #           print(f'Fetching {len(id_list)} abstracts...')\n",
        "    #           handle = Entrez.efetch(db='pubmed', id=id_list, retmode='xml')\n",
        "    #           records = Entrez.read(handle)\n",
        "    #           handle.close()\n",
        "    #           return records\n",
        "    #       except IncompleteRead:\n",
        "    #           print(f'Error fetching abstracts, retrying ({i+1}/5)...')\n",
        "    #           time.sleep(5) # Wait 5 seconds before retrying\n",
        "    #           raise Exception('Failed to fetch abstracts after 5 attempts.')\n",
        "\n",
        "    # fetch_abstracts(id_list)\n",
        "\n",
        "        # Extract abstracts and date information for each record\n",
        "        for record in records['PubmedArticle']:\n",
        "            try:\n",
        "                abstract = record['MedlineCitation']['Article']['Abstract']['AbstractText'][0]\n",
        "            except (KeyError, IndexError):\n",
        "                abstract = 'Not available'\n",
        "            #EntrezDate\n",
        "            try:\n",
        "                pub_date = record['MedlineCitation']['DateRevised']\n",
        "                pub_date_str = f\"{pub_date.get('Year', 'Not available')}-{pub_date.get('Month', 'Not available')}-{pub_date.get('Day', 'Not available')}\"\n",
        "            except KeyError:\n",
        "                pub_date_str = 'Not available'\n",
        "            #PubDate\n",
        "            try:\n",
        "                pub_date1 = record['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']\n",
        "                pub_date_str1 = f\"{pub_date1.get('Year', 'Not available')} {pub_date1.get('Month', 'Not available')}\"\n",
        "            except KeyError:\n",
        "                pub_date_str1 = 'Not available'\n",
        "            try:\n",
        "                journal = record['MedlineCitation']['Article']['Journal']['Title']\n",
        "            except KeyError:\n",
        "                journal = 'Not available'\n",
        "            try:\n",
        "                authors = record['MedlineCitation']['Article']['AuthorList']\n",
        "                author_names = [f\"{author.get('LastName', 'Not available')}, {author.get('ForeName', '')}\" for author in authors]\n",
        "                authors_str = ', '.join(author_names)\n",
        "            except KeyError:\n",
        "                authors_str = 'Not available'\n",
        "            try:\n",
        "                pmid = record['MedlineCitation']['PMID']\n",
        "            except KeyError:\n",
        "                pmid = 'Not available'\n",
        "            \n",
        "            #Add new PubMed ID to set\n",
        "            skipme=1\n",
        "            if pmid not in seen_pmids:\n",
        "                seen_pmids.add(pmid)\n",
        "                skipme=0\n",
        "\n",
        "            \n",
        "\n",
        "            Date1 = pub_date_str1[0:4]\n",
        "            Date0 = pub_date_str[0:4]\n",
        "\n",
        "            \n",
        "\n",
        "            if Date1 != Date0:\n",
        "                if \"Not\" in Date1:\n",
        "                    out_string = f\"{pub_date_str}  -DateCatalogued\\n{pub_date_str1} -DatePublished\\nAuthors: {authors_str}\\nJournal: {journal}\\nTitle: {record['MedlineCitation']['Article']['ArticleTitle']}\\nPMID: {pmid}\\nAbstract: {abstract}\\n\\n\"\n",
        "                else:\n",
        "                    if Date1 < Date0:\n",
        "                        out_string = f\"{pub_date_str1} -DatePublished\\n{pub_date_str}  -DateCatalogued\\nAuthors: {authors_str}\\nJournal: {journal}\\nTitle: {record['MedlineCitation']['Article']['ArticleTitle']}\\nPMID: {pmid}\\nAbstract: {abstract}\\n\\n\"    \n",
        "                    if Date1 > Date0:\n",
        "                        out_string = f\"{pub_date_str}  -DateCatalogued\\n{pub_date_str1} -DatePublished\\nAuthors: {authors_str}\\nJournal: {journal}\\nTitle: {record['MedlineCitation']['Article']['ArticleTitle']}\\nPMID: {pmid}\\nAbstract: {abstract}\\n\\n\"\n",
        "            else:\n",
        "                out_string = f\"{pub_date_str}  -DateCatalogued\\n{pub_date_str1} -DatePublished\\nAuthors: {authors_str}\\nJournal: {journal}\\nTitle: {record['MedlineCitation']['Article']['ArticleTitle']}\\nPMID: {pmid}\\nAbstract: {abstract}\\n\\n\"\n",
        "            \n",
        "            if skipme==0:\n",
        "                abstracts_with_info.append(out_string)\n",
        "                \n",
        "\n",
        "\n",
        "        time.sleep(1) # Add a delay of 1 second\n",
        "    else:\n",
        "        print('No results found for this group.')\n",
        "\n",
        "# Sort abstracts by date\n",
        "abstracts_with_info.sort(reverse=True)\n",
        "\n",
        "# Create subfolder if it doesn't exist\n",
        "if not os.path.exists(\"phyto_results\"):\n",
        "    os.mkdir(\"phyto_results\")\n",
        "\n",
        "# Get current time to name output file\n",
        "user_query = re.sub(r'[^a-zA-Z0-9]+', '_', user_query)\n",
        "now = datetime.datetime.now()\n",
        "\n",
        "#output_file_name = f\"phyto_results/{user_query}_{now.strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "# experimental save file to Google Drive\n",
        "output_file_name = f\"drive/MyDrive/plant_search_text_files/saved_searches/{choice}_{user_query}_{now.strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "\n",
        "# Merge all abstracts into one file, sorted by date\n",
        "with open(output_file_name, 'w', encoding='utf-8') as out_file, \\\n",
        "     open('pubmed_query.txt', 'w', encoding='utf-8') as query_file:\n",
        "    query_file.write(query_terms + '\\n')\n",
        "    count_papers = 1\n",
        "    total_length = len(abstracts_with_info)\n",
        "    out_file.write(str(total_length) + ' papers are in this text file \\n' )\n",
        "    for abstract in abstracts_with_info:\n",
        "        out_file.write('Paper #' + str(count_papers) + ' - ')\n",
        "        count_papers += 1\n",
        "        out_file.write(abstract)\n",
        "        \n",
        "\n",
        "# Empty the input_files folder\n",
        "for file_name in os.listdir('input_files'):\n",
        "    file_path = os.path.join('input_files', file_name)\n",
        "    try:\n",
        "        os.remove(file_path)\n",
        "    except:\n",
        "        print(f'Error deleting {file_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg425fwdWLow",
        "outputId": "e6c81b9c-8c2b-4ef4-93fd-9dacfb1cd974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is your email address?bclarky12@gmail.com\n",
            "Do you want to search over all plant genera (enter 1) | all phytochemicals (enter 2) | or both (enter 3) |                If you do not want to search over plants or phytochemicals,                 try searching over human genes first (not as good as MESH term search) (enter 4) : 1\n",
            "Enter additional non-plant search terms: \"interferon gamma\"\n",
            "Searching group 1/38\n",
            "Fetching 334 abstracts...\n",
            "Searching group 2/38\n",
            "Fetching 319 abstracts...\n",
            "Searching group 3/38\n",
            "Fetching 130 abstracts...\n",
            "Searching group 4/38\n",
            "Fetching 323 abstracts...\n",
            "Searching group 5/38\n",
            "Fetching 306 abstracts...\n",
            "Searching group 6/38\n",
            "Fetching 215 abstracts...\n",
            "Searching group 7/38\n",
            "Fetching 298 abstracts...\n",
            "Searching group 8/38\n",
            "Fetching 296 abstracts...\n",
            "Searching group 9/38\n",
            "Fetching 330 abstracts...\n",
            "Searching group 10/38\n",
            "Fetching 308 abstracts...\n",
            "Searching group 11/38\n",
            "Fetching 36 abstracts...\n",
            "Searching group 12/38\n",
            "Fetching 240 abstracts...\n",
            "Searching group 13/38\n",
            "Fetching 1656 abstracts...\n",
            "Searching group 14/38\n",
            "Fetching 154 abstracts...\n",
            "Searching group 15/38\n",
            "Fetching 360 abstracts...\n",
            "Searching group 16/38\n",
            "Fetching 113 abstracts...\n",
            "Searching group 17/38\n",
            "Fetching 170 abstracts...\n",
            "Searching group 18/38\n",
            "Fetching 395 abstracts...\n",
            "Searching group 19/38\n",
            "Fetching 100 abstracts...\n",
            "Searching group 20/38\n",
            "Fetching 46 abstracts...\n",
            "Searching group 21/38\n",
            "Fetching 268 abstracts...\n",
            "Searching group 22/38\n",
            "Fetching 191 abstracts...\n",
            "Searching group 23/38\n",
            "Fetching 90 abstracts...\n",
            "Searching group 24/38\n",
            "Fetching 101 abstracts...\n",
            "Searching group 25/38\n",
            "Fetching 66 abstracts...\n",
            "Searching group 26/38\n",
            "Fetching 107 abstracts...\n",
            "Searching group 27/38\n",
            "Fetching 332 abstracts...\n",
            "Searching group 28/38\n",
            "Fetching 100 abstracts...\n",
            "Searching group 29/38\n",
            "Fetching 129 abstracts...\n",
            "Searching group 30/38\n",
            "Fetching 129 abstracts...\n",
            "Searching group 31/38\n",
            "Fetching 287 abstracts...\n",
            "Searching group 32/38\n",
            "Fetching 252 abstracts...\n",
            "Searching group 33/38\n",
            "Fetching 169 abstracts...\n",
            "Searching group 34/38\n",
            "Fetching 192 abstracts...\n",
            "Searching group 35/38\n",
            "Fetching 174 abstracts...\n",
            "Searching group 36/38\n",
            "Fetching 267 abstracts...\n",
            "Searching group 37/38\n",
            "Fetching 237 abstracts...\n",
            "Searching group 38/38\n",
            "Fetching 8 abstracts...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n8LZ1dMJsd06"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}